{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSV파일을 이용하여 유튜브에서 음원추출"
      ],
      "metadata": {
        "id": "EOX6rEFx-aTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFhOZgxB9-Vg"
      },
      "outputs": [],
      "source": [
        "!pip install - pytube\n",
        "!pip install -q pycaret"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import time\n",
        "import os\n",
        "import librosa, librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "import sklearn\n",
        "import random\n",
        "from pycaret.classification import *\n",
        "from sklearn.metrics import *\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n"
      ],
      "metadata": {
        "id": "1o8yq85i-IYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "vrRWcZeAXrIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "Reo2kTDDXr9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RGTx3yIyXUEq",
        "outputId": "78530718-a1f6-4b6b-e267-782512262354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EFX1AlngXWvq",
        "outputId": "e6ac2d91-9648-49eb-b62c-209294c684a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 파일 불러오기\n",
        "def load_music(dir_path):\n",
        "    dir_path = dir_path\n",
        "    global music_path\n",
        "    music_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            music_path.append(os.path.join(root, file))\n",
        "    music_path.sort()\n",
        "load_music('/content/drive/MyDrive/dataton/csv수집/new_csv')"
      ],
      "metadata": {
        "id": "oiVG52ZH-Kot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 음악 저장 폴더 생성 없을 시 실행\n",
        "# def make_folder():\n",
        "#     for i in range(len(music_path)):\n",
        "#         os.mkdir(f\"/content/drive/MyDrive/dataton/csv수집/new_music/{music_path[i].split('/')[-1].split('.')[0]}\")\n",
        "# make_folder()"
      ],
      "metadata": {
        "id": "hO4-w2dg-LFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 음악 저장 경로\n",
        "def load_folder(dir_path):\n",
        "    dir_path = dir_path\n",
        "    global file_path\n",
        "    file_path = list()\n",
        "\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for directorie in directories:\n",
        "            file_path.append(os.path.join(root, directorie))\n",
        "    file_path.sort()\n",
        "load_folder('/content/drive/MyDrive/dataton/csv수집/new_music')\n"
      ],
      "metadata": {
        "id": "jmv_X6zn-N8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mp3 파일을 wav 파일 30초로 변환\n",
        "def split_and_save(wav_path, output_dir, segment_length=30):\n",
        "\n",
        "    \"\"\"\n",
        "    - wav_path : 로드할 WAV 파일 또는 MP3 파일의 경로\n",
        "    - output_dir : WAV 파일들을 저장할 디렉토리 경로\n",
        "    - segment_length : 분할할 세그먼트의 길이 (초 단위, 기본값은 30초)\n",
        "    \"\"\"\n",
        "\n",
        "    # 출력 디렉토리가 존재하지 않으면 생성\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    y, sr = librosa.load(wav_path, sr=None)\n",
        "    # 샘플의 총 길이 계산\n",
        "    total_samples = len(y)\n",
        "    # 전체 오디오를 segment_length 초 단위로 나눈 각 세그먼트의 샘플 수 계산\n",
        "    output_path = f\"{output_dir}/segment_{i}.wav\"\n",
        "    sf.write(output_path, y[0:sr*120], sr)"
      ],
      "metadata": {
        "id": "qqdZL0e8-Rit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유툽에서 mp4 파일 다운\n",
        "def download_music(file_path,music_path):\n",
        "    global i\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df[['youtube_url']].dropna(axis = 0).reset_index(drop = True)\n",
        "    url = df['youtube_url'].to_list()\n",
        "    word = 'youtube'\n",
        "    ben = [i for i in range(len(url)) if word not in url[i]]\n",
        "    for i in ben:\n",
        "        del url[i]\n",
        "\n",
        "    i = 0\n",
        "    while i < (len(url)):\n",
        "      yt = YouTube(url[i])\n",
        "\n",
        "      i +=1\n",
        "      name = str(i) + \".mp4\"\n",
        "      try:\n",
        "        music = yt.streams.filter(only_audio=True, audio_codec=\"mp4a.40.5\" if True else \"mp4a.40.2\").first()\n",
        "        music.download(output_path=music_path, filename=name)\n",
        "        os.rename(f'{music_path}/{name}',f'{music_path}/{i}.mp3')\n",
        "        # os.remove(f'{file_path}/{name}')\n",
        "        split_and_save(f'{music_path}/{str(i)}.mp3', music_path, segment_length=30)\n",
        "      except:\n",
        "        print(f'{i}번째 오류')"
      ],
      "metadata": {
        "id": "Yd_h_IWQ-TIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다운로드\n",
        "def full_download():\n",
        "    load_music()\n",
        "    load_folder()\n",
        "    # print(file_path[1].split('/')[-1])\n",
        "    # download_music(music_path[1],file_path[1])\n",
        "    for i in range(len(file_path)):\n",
        "        print(file_path[i].split('/')[-1])\n",
        "        download_music(music_path[i],file_path[i])"
      ],
      "metadata": {
        "id": "aSM9BjAW-UYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "full_download()"
      ],
      "metadata": {
        "id": "BO7wO5oc-V-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wav파일에서 피처 추출"
      ],
      "metadata": {
        "id": "d13sLiKeBsXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 스펙트럴"
      ],
      "metadata": {
        "id": "inTDErg4I-rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 데이터 가져오기(폴더 정보)\n",
        "def load_path():\n",
        "    dir_path = '/content/drive/MyDrive/dataton/csv수집/2minute'\n",
        "    global file_path\n",
        "    file_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for directorie in directories:\n",
        "            file_path.append(os.path.join(root, directorie))\n",
        "    file_path.sort()\n",
        "# 데이터 가져오기(파일 정보)\n",
        "def load_music(file_path):\n",
        "    dir_path = file_path\n",
        "    global music_path\n",
        "    music_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            music_path.append(os.path.join(root, file))\n",
        "    music_path.sort()\n",
        "\n",
        "# 리스트 생성\n",
        "def make_list_spec():\n",
        "    global rolloff_mean, rolloff_var, centroid_mean, centroid_var,flux_mean,flux_var, contrast_mean, contrast_var, flatness_mean,flatness_var,bw_mean,bw_var,chroma_mean, chroma_var,\\\n",
        "    mfcc1_mean, mfcc1_var,mfcc2_mean,mfcc2_var,mfcc3_mean, mfcc3_var,mfcc4_mean, mfcc4_var,mfcc5_mean, mfcc5_var,mfcc6_mean, \\\n",
        "    mfcc6_var,mfcc7_mean, mfcc7_var,mfcc8_mean, mfcc8_var,mfcc9_mean, mfcc9_var,mfcc10_mean, mfcc10_var,mfcc11_mean, mfcc11_var,mfcc12_mean, mfcc12_var,mfcc13_mean, \\\n",
        "    mfcc13_var,mfcc14_mean, mfcc14_var,mfcc15_mean, mfcc15_var,mfcc16_mean, mfcc16_var,mfcc17_mean, mfcc17_var,mfcc18_mean, mfcc18_var,mfcc19_mean, mfcc19_var,mfcc20_mean, mfcc20_var,\\\n",
        "    label, name\n",
        "    rolloff_mean = list()\n",
        "    rolloff_var = list()\n",
        "    centroid_mean = list()\n",
        "    centroid_var = list()\n",
        "    flux_mean = list()\n",
        "    flux_var = list()\n",
        "    contrast_mean = list()\n",
        "    contrast_var = list()\n",
        "    flatness_mean = list()\n",
        "    flatness_var = list()\n",
        "    bw_mean = list()\n",
        "    bw_var = list()\n",
        "    chroma_mean = list()\n",
        "    chroma_var = list()\n",
        "    label = list()\n",
        "    name = list()\n",
        "    mfcc1_mean = list()\n",
        "    mfcc1_var = list()\n",
        "    mfcc2_mean = list()\n",
        "    mfcc2_var = list()\n",
        "    mfcc3_mean = list()\n",
        "    mfcc3_var = list()\n",
        "    mfcc4_mean = list()\n",
        "    mfcc4_var = list()\n",
        "    mfcc5_mean = list()\n",
        "    mfcc5_var = list()\n",
        "    mfcc6_mean = list()\n",
        "    mfcc6_var = list()\n",
        "    mfcc7_mean = list()\n",
        "    mfcc7_var = list()\n",
        "    mfcc8_mean = list()\n",
        "    mfcc8_var = list()\n",
        "    mfcc9_mean = list()\n",
        "    mfcc9_var = list()\n",
        "    mfcc10_mean = list()\n",
        "    mfcc10_var = list()\n",
        "    mfcc11_mean = list()\n",
        "    mfcc11_var = list()\n",
        "    mfcc12_mean = list()\n",
        "    mfcc12_var = list()\n",
        "    mfcc13_mean = list()\n",
        "    mfcc13_var = list()\n",
        "    mfcc14_mean = list()\n",
        "    mfcc14_var = list()\n",
        "    mfcc15_mean = list()\n",
        "    mfcc15_var = list()\n",
        "    mfcc16_mean = list()\n",
        "    mfcc16_var = list()\n",
        "    mfcc17_mean = list()\n",
        "    mfcc17_var = list()\n",
        "    mfcc18_mean = list()\n",
        "    mfcc18_var = list()\n",
        "    mfcc19_mean = list()\n",
        "    mfcc19_var = list()\n",
        "    mfcc20_mean = list()\n",
        "    mfcc20_var = list()\n",
        "\n",
        "\n",
        "\n",
        "# 스펙트럴 롤오프(Spectral Rolloff) : 주파수 스펙트럼의 하위 85%의 에너지를 포함하는 주파수를 나타냅니다.\n",
        "def get_rolloff(y,sr):\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "    rolloff_mean.append(spectral_rolloff.mean())\n",
        "    rolloff_var.append(spectral_rolloff.var())\n",
        "\n",
        "# centroid (소리를 주파수 표현했을 때, 주파수의 가중평균을 계산하여 소리의 \"무게 중심\"이 어딘지를 알려주는 지표)\n",
        "def get_centroid(y,sr):\n",
        "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "    centroid_mean.append(spectral_centroids.mean())\n",
        "    centroid_var.append(spectral_centroids.var())\n",
        "\n",
        "# 스펙트럴 플럭스(Spectral Flux) : 스펙트럼의 변화량을 나타내며, 음향 신호의 동적인 변화를 측정합니다.\n",
        "def get_flux(y,sr):\n",
        "    spectral_flux = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    flux_mean.append(spectral_flux.mean())\n",
        "    flux_var.append(spectral_flux.var())\n",
        "\n",
        "# 스펙트럴 콘트라스트(Spectral Contrast) : 스펙트럼의 피크와 계곡의 차이를 나타냅니다.\n",
        "def get_contrast(y,sr):\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    contrast_mean.append(spectral_contrast.mean())\n",
        "    contrast_var.append(spectral_contrast.var())\n",
        "\n",
        "# Spectral Flatness : 스펙트럼이 얼마나 평탄한지를 나타내는 지표\n",
        "def get_flatness(y,sr):\n",
        "    D = np.abs(librosa.stft(y))\n",
        "    flatness = librosa.feature.spectral_flatness(S=D)[0]\n",
        "    flatness_mean.append(flatness.mean())\n",
        "    flatness_var.append(flatness.var())\n",
        "\n",
        "#  크로마 피처 (Chroma Features) 추출 : 크로마 피처는 음악의 하모닉과 멜로딕 정보를 캡처합니다.\n",
        "def get_chroma(y,sr):\n",
        "    chromagram = librosa.feature.chroma_stft(y=y, sr=sr).flatten()\n",
        "    chroma_mean.append(chromagram.mean())\n",
        "    chroma_var.append(chromagram.var())\n",
        "\n",
        "\n",
        "# bandwidth (대역폭)\n",
        "def get_bandwith(y,sr):\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr).flatten()\n",
        "    bw_mean.append(spec_bw.mean())\n",
        "    bw_var.append(spec_bw.var())\n",
        "\n",
        "\n",
        "def get_mfcc(y,sr):\n",
        "    mfccs = librosa.feature.mfcc(y= y, sr=sr)\n",
        "    mfcc1_mean.append(mfccs[0].mean())\n",
        "    mfcc1_var.append(mfccs[0].var())\n",
        "    mfcc2_mean.append(mfccs[1].mean())\n",
        "    mfcc2_var.append(mfccs[1].var())\n",
        "    mfcc3_mean.append(mfccs[2].mean())\n",
        "    mfcc3_var.append(mfccs[2].var())\n",
        "    mfcc4_mean.append(mfccs[3].mean())\n",
        "    mfcc4_var.append(mfccs[3].var())\n",
        "    mfcc5_mean.append(mfccs[4].mean())\n",
        "    mfcc5_var.append(mfccs[4].var())\n",
        "    mfcc6_mean.append(mfccs[5].mean())\n",
        "    mfcc6_var.append(mfccs[5].var())\n",
        "    mfcc7_mean.append(mfccs[6].mean())\n",
        "    mfcc7_var.append(mfccs[6].var())\n",
        "    mfcc8_mean.append(mfccs[7].mean())\n",
        "    mfcc8_var.append(mfccs[7].var())\n",
        "    mfcc9_mean.append(mfccs[8].mean())\n",
        "    mfcc9_var.append(mfccs[8].var())\n",
        "    mfcc10_mean.append(mfccs[9].mean())\n",
        "    mfcc10_var.append(mfccs[9].var())\n",
        "    mfcc11_mean.append(mfccs[10].mean())\n",
        "    mfcc11_var.append(mfccs[10].var())\n",
        "    mfcc12_mean.append(mfccs[11].mean())\n",
        "    mfcc12_var.append(mfccs[11].var())\n",
        "    mfcc13_mean.append(mfccs[12].mean())\n",
        "    mfcc13_var.append(mfccs[12].var())\n",
        "    mfcc14_mean.append(mfccs[13].mean())\n",
        "    mfcc14_var.append(mfccs[13].var())\n",
        "    mfcc15_mean.append(mfccs[14].mean())\n",
        "    mfcc15_var.append(mfccs[14].var())\n",
        "    mfcc16_mean.append(mfccs[15].mean())\n",
        "    mfcc16_var.append(mfccs[15].var())\n",
        "    mfcc17_mean.append(mfccs[16].mean())\n",
        "    mfcc17_var.append(mfccs[16].var())\n",
        "    mfcc18_mean.append(mfccs[17].mean())\n",
        "    mfcc18_var.append(mfccs[17].var())\n",
        "    mfcc19_mean.append(mfccs[18].mean())\n",
        "    mfcc19_var.append(mfccs[18].var())\n",
        "    mfcc20_mean.append(mfccs[19].mean())\n",
        "    mfcc20_var.append(mfccs[19].var())\n",
        "\n",
        "\n",
        "# label (target(mood))\n",
        "def get_label(file_path):\n",
        "    mood = file_path.split('/')[-1].split('(')[0]\n",
        "    label.append(mood)\n",
        "\n",
        "# 파일명 호출\n",
        "def file_name(music_path):\n",
        "    names = music_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
        "    name.append(names)\n",
        "\n",
        "#\n",
        "def make_DataFrame():\n",
        "    cnt = 0\n",
        "    load_path()\n",
        "    make_list_spec()\n",
        "    for i in range(len(file_path)):\n",
        "        print(file_path[i].split('/')[-1].split('(')[0])\n",
        "        load_music(file_path[i])\n",
        "        for j in range(len(music_path)):\n",
        "            get_label(file_path[i])\n",
        "            file_name(music_path[j])\n",
        "            y, sr = librosa.load(music_path[j])\n",
        "            get_rolloff(y,sr)\n",
        "            get_centroid(y,sr)\n",
        "            get_flux(y,sr)\n",
        "            get_contrast(y,sr)\n",
        "            get_mfcc(y,sr)\n",
        "            get_bandwith(y,sr)\n",
        "            get_chroma(y,sr)\n",
        "            get_flatness(y,sr)\n",
        "            cnt+=1\n",
        "            if cnt % 100 == 0:\n",
        "                print(cnt)\n",
        "\n",
        "    df = pd.DataFrame({'name':name,'target':label,'rolloff_mean':rolloff_mean, 'rolloff_var':rolloff_var, 'centroid_mean':centroid_mean, 'centroid_var':centroid_var,'flux_mean':flux_mean,\n",
        "                       'flux_var':flux_var, 'contrast_mean':contrast_mean, 'contrast_var':contrast_var,'flatness_mean':flatness_mean,'flatness_var': flatness_var,'bw_mean':bw_mean,'bw_var':bw_var,\n",
        "                       'chroma_mean':chroma_mean, 'chroma_var':chroma_var, 'mfcc1_mean':mfcc1_mean,'mfcc1_var':mfcc1_var,'mfcc2_mean':mfcc2_mean,'mfcc2_var':mfcc2_var,'mfcc3_mean':mfcc3_mean,\n",
        "                       'mfcc3_var':mfcc3_var,'mfcc4_mean':mfcc4_mean,'mfcc4_var':mfcc4_var,'mfcc5_mean':mfcc5_mean,'mfcc5_var':mfcc5_var,'mfcc6_mean':mfcc6_mean,'mfcc6_var':mfcc6_var,'mfcc7_mean':mfcc7_mean,\n",
        "                       'mfcc7_var':mfcc7_var,'mfcc8_mean':mfcc8_mean,'mfcc8_var':mfcc8_var,'mfcc9_mean':mfcc9_mean,'mfcc9_var':mfcc9_var,'mfcc10_mean':mfcc10_mean,'mfcc10_var':mfcc10_var,\n",
        "                       'mfcc11_mean':mfcc11_mean,'mfcc11_var':mfcc11_var,'mfcc12_mean':mfcc12_mean,'mfcc12_var':mfcc12_var,'mfcc13_mean':mfcc13_mean,'mfcc13_var':mfcc13_var,'mfcc14_mean':mfcc14_mean,\n",
        "                       'mfcc14_var':mfcc14_var,'mfcc15_mean':mfcc15_mean,'mfcc15_var':mfcc15_var,'mfcc16_mean':mfcc16_mean,'mfcc16_var':mfcc16_var,'mfcc17_mean':mfcc17_mean,'mfcc17_var':mfcc17_var,\n",
        "                       'mfcc18_mean':mfcc18_mean,'mfcc18_var':mfcc18_var,'mfcc19_mean':mfcc19_mean,'mfcc19_var':mfcc19_var,'mfcc20_mean':mfcc20_mean,'mfcc20_var':mfcc20_var})\n",
        "    return df\n",
        "a = make_DataFrame()"
      ],
      "metadata": {
        "id": "SBqeUMZCu_Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfkCvUpBAIBR"
      },
      "source": [
        "## 음향특징 추출"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 데이터 가져오기(폴더 정보)\n",
        "def load_path():\n",
        "    dir_path = '/content/drive/MyDrive/dataton/csv수집/2minute'\n",
        "    global file_path\n",
        "    file_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for directorie in directories:\n",
        "            file_path.append(os.path.join(root, directorie))\n",
        "    file_path.sort()\n",
        "# 데이터 가져오기(파일 정보)\n",
        "def load_music(file_path):\n",
        "    dir_path = file_path\n",
        "    global music_path\n",
        "    music_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            music_path.append(os.path.join(root, file))\n",
        "    music_path.sort()\n",
        "\n",
        "def make_list_acoustic():\n",
        "    global rms_mean, rms_var, zero_crossings_mean, zero_crossings_var, tempo, label, name\n",
        "    rms_mean= list()\n",
        "    rms_var = list()\n",
        "    zero_crossings_mean = list()\n",
        "    zero_crossings_var = list()\n",
        "    tempo = list()\n",
        "    label = list()\n",
        "    name = list()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_label(file_path):\n",
        "    mood = file_path.split('/')[-1].split('(')[0]\n",
        "    label.append(mood)\n",
        "\n",
        "def file_name(music_path):\n",
        "    names = music_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
        "    name.append(names)\n",
        "\n",
        "\n",
        "\n",
        "# 루트 평균 제곱(RMS) 에너지 : RMS 에너지는 신호의 에너지를 나타내며, 신호의 진폭을 측정하는 데 사용됩니다.\n",
        "def get_rms(y):\n",
        "    rmse = librosa.feature.rms(y=y)[0]\n",
        "    rms_mean.append(rmse.mean())\n",
        "    rms_var.append(rmse.var())\n",
        "\n",
        "#  제로 크로싱율 (Zero-Crossing Rate) : 제로 크로싱율은 신호가 0을 교차하는 횟수를 나타내며, 음의 거칠기나 노이즈를 측정하는 데 사용됩니다.\n",
        "def get_zero_crossing_rate(y):\n",
        "    zero_crossings = librosa.zero_crossings(y, pad=False)\n",
        "    zero_crossings_mean.append(zero_crossings.mean())\n",
        "    zero_crossings_var.append(zero_crossings.var())\n",
        "\n",
        "# 음악의 템포 추출\n",
        "def get_tempo(y,sr):\n",
        "    tempos, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "    tempo.append(tempos[0])\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "def make_DataFrame():\n",
        "    cnt = 0\n",
        "    load_path()\n",
        "    make_list_acoustic()\n",
        "    for i in range(len(file_path)):\n",
        "        print(file_path[i].split('/')[-1].split('(')[0])\n",
        "        load_music(file_path[i])\n",
        "        for j in range(len(music_path)):\n",
        "            get_label(file_path[i])\n",
        "            file_name(music_path[j])\n",
        "            y, sr = librosa.load(music_path[j])\n",
        "            get_zero_crossing_rate(y)\n",
        "            get_rms(y)\n",
        "            get_tempo(y,sr)\n",
        "            cnt+=1\n",
        "            if cnt % 100 == 0:\n",
        "                print(cnt)\n",
        "    df = pd.DataFrame({'name':name, 'target':label, 'rms_mean':rms_mean, 'rms_var':rms_var, 'zero_crossings_mean':zero_crossings_mean, 'zero_crossings_var':zero_crossings_var,'tempo':tempo,})\n",
        "    return df\n",
        "b = make_DataFrame()"
      ],
      "metadata": {
        "id": "P8GZ6xB_vl-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV1jvIOIB3xl"
      },
      "source": [
        "## 음악 구조적 특징 추출"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 데이터 가져오기(폴더 정보)\n",
        "def load_path():\n",
        "    dir_path = '/content/drive/MyDrive/dataton/csv수집/2minute'\n",
        "    global file_path\n",
        "    file_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for directorie in directories:\n",
        "            file_path.append(os.path.join(root, directorie))\n",
        "    file_path.sort()\n",
        "# 데이터 가져오기(파일 정보)\n",
        "def load_music(file_path):\n",
        "    dir_path = file_path\n",
        "    global music_path\n",
        "    music_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            music_path.append(os.path.join(root, file))\n",
        "    music_path.sort()\n",
        "\n",
        "def make_list_musical():\n",
        "    global beat_mean, beat_var, hamonic_mean,hamonic_var,perc_mean,perc_var, label, name\n",
        "    beat_mean= list()\n",
        "    beat_var = list()\n",
        "    zero_crossings_mean = list()\n",
        "    zero_crossings_var = list()\n",
        "    hamonic_mean = list()\n",
        "    hamonic_var = list()\n",
        "    perc_mean = list()\n",
        "    perc_var = list()\n",
        "    label = list()\n",
        "    name = list()\n",
        "\n",
        "def get_label(file_path):\n",
        "    mood = file_path.split('/')[-1].split('(')[0]\n",
        "    label.append(mood)\n",
        "\n",
        "def file_name(music_path):\n",
        "    names = music_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
        "    name.append(names)\n",
        "\n",
        "# 리듬 패턴 (Rhythm Patterns) 추출 : 리듬 패턴을 추출하기 위해 오디오 신호의 주기성을 분석합니다.\n",
        "def get_phythm(y,sr):\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
        "    beat_times = librosa.frames_to_time(beats, sr=sr)\n",
        "    beat_mean.append(beat_times.mean())\n",
        "    beat_var.append(beat_times.var())\n",
        "\n",
        "# Hamonic,Percussive_Components (hamonic : 사람의 귀로 구분할 수 없는 특징들(음악의 색깔), Percussives: 리듬과 감정을 나타내는 충격파)\n",
        "def get_Hamonic_and_Percussive_Components(y):\n",
        "    y_harm, y_perc = librosa.effects.hpss(y)\n",
        "    hamonic_mean.append(y_harm.mean())\n",
        "    hamonic_var.append(y_harm.var())\n",
        "    perc_mean.append(y_perc.mean())\n",
        "    perc_var.append(y_perc.var())\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "def make_DataFrame():\n",
        "    cnt = 0\n",
        "    load_path()\n",
        "    make_list_musical()\n",
        "    for i in range(len(file_path)):\n",
        "        print(file_path[i].split('/')[-1].split('(')[0])\n",
        "        load_music(file_path[i])\n",
        "        for j in range(len(music_path)):\n",
        "            get_label(file_path[i])\n",
        "            file_name(music_path[j])\n",
        "            y, sr = librosa.load(music_path[j])\n",
        "            get_phythm(y,sr)\n",
        "            get_Hamonic_and_Percussive_Components(y)\n",
        "            cnt+=1\n",
        "            if cnt % 100 == 0:\n",
        "                print(cnt)\n",
        "    df = pd.DataFrame({'name':name, 'target':label,'beat_mean':beat_mean, 'beat_var':beat_var, 'hamonic_mean':hamonic_mean,'hamonic_var':hamonic_var,'perc_mean':perc_mean,'perc_var':perc_var})\n",
        "    return df\n",
        "\n",
        "c = make_DataFrame()"
      ],
      "metadata": {
        "id": "Xu9ZDG2CvehW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 멜 스펙토그램 CNN"
      ],
      "metadata": {
        "id": "CYzKnPPwt0r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"running on the CPU\")"
      ],
      "metadata": {
        "id": "YZnUqnMGt2kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    \"SR\":22050,\n",
        "    \"SEED\":42,\n",
        "    \"BATCH_SIZE\":32,\n",
        "    \"TOTAL_BATCH_SIZE\":32,\n",
        "    \"EPOCHS\":100,\n",
        "    \"LR\":1e-4,\n",
        "}"
      ],
      "metadata": {
        "id": "KrvLU2dPKl8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ],
      "metadata": {
        "id": "uQlG_gXqt34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_path():\n",
        "    dir_path = r\"C:\\Users\\alsgu\\OneDrive\\바탕 화면\\새 폴더\"\n",
        "    global file_path\n",
        "    file_path = list()\n",
        "\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for directorie in directories:\n",
        "            file_path.append(os.path.join(root, directorie))\n",
        "    file_path.sort()\n",
        "load_path()\n",
        "def load_music(file_path):\n",
        "    dir_path = file_path\n",
        "    global music_path\n",
        "    music_path = list()\n",
        "    for (root,directories, files) in os.walk(dir_path):\n",
        "        for file in files:\n",
        "            music_path.append(os.path.join(root, file))\n",
        "    music_path.sort()"
      ],
      "metadata": {
        "id": "aRcRmikgt6g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = []\n",
        "path = []\n",
        "for i in file_path:\n",
        "    load_music(i)\n",
        "    len(music_path)\n",
        "    for j in music_path:\n",
        "        path.append(j)\n",
        "        label.append(i.split(\"\\\\\")[-1])"
      ],
      "metadata": {
        "id": "HZw70-Lrt7To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.DataFrame({'path': path, 'label':label})\n",
        "df"
      ],
      "metadata": {
        "id": "Xp2S2am7t8bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "cKam7uwNt9Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['path'], df['label'], test_size = 0.2, random_state=42, stratify=df['label'])"
      ],
      "metadata": {
        "id": "GQOdSlMJt-Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([X_train,y_train], axis=1).reset_index(drop = True)\n",
        "test = pd.concat([X_test, y_test], axis=1).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "5SnEytPst_DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dataset():\n",
        "    dataset = []\n",
        "    for i in tqdm(range(len(train['path'])),colour='green'):\n",
        "        if 'wav' in train['path'][i]:\n",
        "            data, sr = librosa.load(train['path'][i], sr = 22050)\n",
        "            class_label = train['label'][i]\n",
        "            dataset.append([data,class_label])\n",
        "\n",
        "    print(\"Dataset 생성 완료\")\n",
        "    return pd.DataFrame(dataset,columns=['data','label'])"
      ],
      "metadata": {
        "id": "jZ95Busjt_rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset():\n",
        "    dataset = []\n",
        "    for i in tqdm(range(len(test['path'])),colour='green'):\n",
        "        if 'wav' in test['path'][i]:\n",
        "            data, sr = librosa.load(test['path'][i], sr = 22050)\n",
        "            class_label = test['label'][i]\n",
        "            dataset.append([data,class_label])\n",
        "\n",
        "    print(\"Dataset 생성 완료\")\n",
        "    return pd.DataFrame(dataset,columns=['data','label'])"
      ],
      "metadata": {
        "id": "nduOr4KxuATX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_wav = train_dataset()\n",
        "test_wav = test_dataset()"
      ],
      "metadata": {
        "id": "HkLDhovBuBJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.array(train_wav.data)\n",
        "test_x = np.array(test_wav.data)"
      ],
      "metadata": {
        "id": "4qqohJ92uBzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mini(data):\n",
        "\n",
        "    mini = 9999999\n",
        "    for i in data:\n",
        "        if len(i) < mini:\n",
        "            mini = len(i)\n",
        "\n",
        "    return mini\n",
        "\n",
        "train_mini = get_mini(train_x)\n",
        "test_mini = get_mini(test_x)\n",
        "\n",
        "#음성들의 길이를 맞춰줍니다.\n",
        "\n",
        "mini = np.min([train_mini, test_mini])"
      ],
      "metadata": {
        "id": "5z109IaIuClQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_length(data, d_mini):\n",
        "\n",
        "    result = []\n",
        "    for i in data:\n",
        "        result.append(i[:d_mini])\n",
        "    result = np.array(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "train_x = set_length(train_x, mini)\n",
        "test_x = set_length(test_x, mini)"
      ],
      "metadata": {
        "id": "9B0rmHTFuDK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train :', train_x.shape)\n",
        "print('test :', test_x.shape)"
      ],
      "metadata": {
        "id": "cal351PwuEBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(data):\n",
        "    mfccs = []\n",
        "    for i in data:\n",
        "        extracted_features = librosa.feature.mfcc(y=i,\n",
        "                                              sr=16000,\n",
        "                                              n_mfcc=40)\n",
        "        mfccs.append(extracted_features)\n",
        "\n",
        "    return mfccs"
      ],
      "metadata": {
        "id": "vWI2vBwNuEmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mfccs = preprocess_dataset(train_x)\n",
        "train_mfccs = np.array(train_mfccs)\n",
        "train_mfccs = train_mfccs.reshape(-1, train_mfccs.shape[1], train_mfccs.shape[2], 1)"
      ],
      "metadata": {
        "id": "KkuPj_p2uFSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_mfccs = preprocess_dataset(test_x)\n",
        "train_mfccs = np.array(train_mfccs)\n",
        "train_mfccs = train_mfccs.reshape(-1, train_mfccs.shape[1], train_mfccs.shape[2], 1)"
      ],
      "metadata": {
        "id": "QUJ3RXusuGy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets # 데이터셋 집합체\n",
        "import torchvision.transforms as transforms # 변환 툴\n",
        "\n",
        "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y, train_mode=True, transforms=None): #필요한 변수들을 선언\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.train_mode = train_mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index): #index번째 data를 return\n",
        "        X = self.X[index]\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            X = self.transforms(X)\n",
        "\n",
        "        if self.train_mode:\n",
        "            y = self.y[index]\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def __len__(self): #길이 return\n",
        "        return len(self.X)"
      ],
      "metadata": {
        "id": "D92fn5kXuHbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_mfccs\n",
        "vali_x = test_mfccs\n",
        "train_y  = train_y\n",
        "vali_y = test_y"
      ],
      "metadata": {
        "id": "-2mrtztPuLlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_x),len(vali_x),len(train_y),len(vali_y)"
      ],
      "metadata": {
        "id": "4_qTsrhXuMU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크 설정\n",
        "num_epochs = 100\n",
        "\n",
        "# 배치 사이즈 설정\n",
        "batch_size = 5\n",
        "\n",
        "#만든 train dataset를 DataLoader에 넣어 batch 만들기\n",
        "train_dataset = CustomDataset(X=train_x, y=train_y)\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "151U-WizuNYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vali_dataset = CustomDataset(X=vali_x, y=vali_y)\n",
        "vali_loader = DataLoader(vali_dataset, batch_size = batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "l3IiB6UUuOG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = len(train_loader)\n",
        "vali_batches = len(vali_loader)\n",
        "\n",
        "print('/ total train batches :', train_batches)\n",
        "print('/ total valid batches :', vali_batches)"
      ],
      "metadata": {
        "id": "2ignv5YcuO_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn # 신경망들이 포함됨\n",
        "\n",
        "class CNNclassification(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNclassification, self).__init__()\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            nn.Conv2d(40, 10, kernel_size=(3,3), stride=1, padding=1), #cnn layer\n",
        "            nn.ReLU(), #activation function\n",
        "            nn.MaxPool2d(kernel_size=(5,5))) #pooling layer\n",
        "\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            nn.Conv2d(10, 100, kernel_size=(3,3), stride=1, padding=1), #cnn layer\n",
        "            nn.ReLU(), #activation function\n",
        "            nn.MaxPool2d(kernel_size=(5,5))) #pooling layer\n",
        "\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            nn.Conv2d(100, 200, kernel_size=(3,3), stride=1, padding=1), #cnn layer\n",
        "            nn.ReLU(), #activation function\n",
        "            nn.MaxPool2d(kernel_size=(5,5))) #pooling layer\n",
        "\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            nn.Conv2d(200, 300, kernel_size=(3,3), stride=1, padding=1), #cnn layer\n",
        "            nn.ReLU(), #activation function\n",
        "            nn.MaxPool2d(kernel_size=(5,5))) #pooling layer\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(48600, 10) #fully connected layer(ouput layer)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.layer1(x) #1층\n",
        "\n",
        "        x = self.layer2(x) #2층\n",
        "\n",
        "        x = self.layer3(x) #3층\n",
        "\n",
        "        x = self.layer4(x) #4층\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1) # N차원 배열 -> 1차원 배열\n",
        "\n",
        "        out = self.fc_layer(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ge-EBffuuRDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "T9p5r2BWuS1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
        "\n",
        "model = CNNclassification().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-3 )\n",
        "scheduler = None"
      ],
      "metadata": {
        "id": "Kq7_3wHTuUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train(model, optimizer, train_loader, scheduler, device):\n",
        "    model.to(device)\n",
        "    n = len(train_loader)\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(1,num_epochs): #에포크 설정\n",
        "        model.train() #모델 학습\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for wav, label in tqdm(iter(train_loader)):\n",
        "\n",
        "            wav, label = wav.to(device), label.to(device) #배치 데이터\n",
        "            optimizer.zero_grad() #배치마다 optimizer 초기화\n",
        "\n",
        "            # Data -> Model -> Output\n",
        "            logit = model(wav) #예측값 산출\n",
        "            loss = criterion(logit, label) #손실함수 계산\n",
        "\n",
        "            # 역전파\n",
        "            loss.backward() #손실함수 기준 역전파\n",
        "            optimizer.step() #가중치 최적화\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(train_loader)))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        #Validation set 평가\n",
        "        model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
        "        vali_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
        "            for wav, label in tqdm(iter(vali_loader)):\n",
        "\n",
        "                wav, label = wav.to(device), label.to(device)\n",
        "                logit = model(wav)\n",
        "                vali_loss += criterion(logit, label)\n",
        "                pred = logit.argmax(dim=1, keepdim=True)  #10개의 class중 가장 값이 높은 것을 예측 label로 추출\n",
        "                correct += pred.eq(label.view_as(pred)).sum().item() #예측값과 실제값이 맞으면 1 아니면 0으로 합산\n",
        "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
        "        print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset)))\n",
        "\n",
        "        #베스트 모델 저장\n",
        "        if best_acc < vali_acc:\n",
        "            best_acc = vali_acc\n",
        "            torch.save(model.state_dict(), './best_model2.pth') #이 디렉토리에 best_model.pth을 저장\n",
        "            print('Model Saved.')"
      ],
      "metadata": {
        "id": "un97VOE_uWOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, train_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "F4HcWhMouW7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 음악 특성 피처 모델"
      ],
      "metadata": {
        "id": "acgPsUKqvtdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spec_df = pd.read_csv('/content/drive/MyDrive/dataton/spectral_30.csv')\n",
        "spec_df.shape, spec_df.columns"
      ],
      "metadata": {
        "id": "20w4cjtovvsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d91faf-fec1-4638-ba38-92917ea6902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3259, 56),\n",
              " Index(['name', 'target', 'rolloff_mean', 'rolloff_var', 'centroid_mean',\n",
              "        'centroid_var', 'flux_mean', 'flux_var', 'contrast_mean',\n",
              "        'contrast_var', 'flatness_mean', 'flatness_var', 'bw_mean', 'bw_var',\n",
              "        'chroma_mean', 'chroma_var', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean',\n",
              "        'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var',\n",
              "        'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean',\n",
              "        'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var',\n",
              "        'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean',\n",
              "        'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var',\n",
              "        'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean',\n",
              "        'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var',\n",
              "        'mfcc20_mean', 'mfcc20_var'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기초 모델"
      ],
      "metadata": {
        "id": "bSwQxxxRwJ-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = spec_df.drop('name', axis = 1).iloc[:,1:]\n",
        "y = spec_df['target']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.1, stratify=y, random_state=100)\n",
        "model = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "                      criterion='gini', max_depth=20, max_features='sqrt',\n",
        "                      max_leaf_nodes=None, max_samples=None,\n",
        "                      min_impurity_decrease=0.0, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      monotonic_cst=None, n_estimators=400, n_jobs=-1,\n",
        "                      oob_score=False, random_state=100, verbose=0,\n",
        "                      warm_start=False)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"훈련 세트 정확도: {:.3f}\".format(model.score(X_train, y_train)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(model.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "Xy9w1vzRwH_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정규화"
      ],
      "metadata": {
        "id": "VvEyv5x7fxA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = spec_df.drop('name', axis = 1).iloc[:,1:]"
      ],
      "metadata": {
        "id": "gmMQCRTMfrkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minmax"
      ],
      "metadata": {
        "id": "3i-nMLXPf24b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "minmax = scaler.fit_transform(X)\n",
        "minmax_df = pd.DataFrame(minmax, columns = X.columns)"
      ],
      "metadata": {
        "id": "GkXB_6xKf2GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = minmax_df\n",
        "y = spec_df['target']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.3, stratify=y, random_state=100)\n",
        "\n",
        "model = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "                      criterion='gini', max_depth=20, max_features='sqrt',\n",
        "                      max_leaf_nodes=None, max_samples=None,\n",
        "                      min_impurity_decrease=0.0, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      monotonic_cst=None, n_estimators=400, n_jobs=-1,\n",
        "                      oob_score=False, random_state=100, verbose=0,\n",
        "                      warm_start=False)\n",
        "# scores = cross_validate(model, X_train, y_train, return_train_score = True, n_jobs = -1)\n",
        "# print(np.mean(scores['train_score']),np.mean(scores['test_score']))\n",
        "model.fit(X_train, y_train)\n",
        "print(\"훈련 세트 정확도: {:.3f}\".format(model.score(X_train, y_train)))\n",
        "print(\"테스트 세트 정확도: {:.3f}\".format(model.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5n1dmtaf-cX",
        "outputId": "aa3d3b62-64a0-4364-9979-97beaabde4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 세트 정확도: 0.982\n",
            "테스트 세트 정확도: 0.536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## std"
      ],
      "metadata": {
        "id": "3ZJ5OdSFgKe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "std_sc = scaler.fit_transform(X)\n",
        "std_df = pd.DataFrame(std_sc, columns = X.columns)"
      ],
      "metadata": {
        "id": "yCcCeeg4gF6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = std_df\n",
        "y = spec_df['target']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.3, stratify=y, random_state=100)\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "model = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "                      criterion='gini', max_depth=20, max_features='sqrt',\n",
        "                      max_leaf_nodes=None, max_samples=None,\n",
        "                      min_impurity_decrease=0.0, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      monotonic_cst=None, n_estimators=400, n_jobs=-1,\n",
        "                      oob_score=False, random_state=100, verbose=0,\n",
        "                      warm_start=False)\n",
        "scores = cross_validate(model, X_train, y_train, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))\n",
        "# model.fit(X_train, y_train)\n",
        "# print(\"훈련 세트 정확도: {:.3f}\".format(model.score(X_train, y_train)))\n",
        "# print(\"테스트 세트 정확도: {:.3f}\".format(model.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNRvNAtUgbR3",
        "outputId": "0ef1c421-2788-4879-b857-6b393f2b09aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9853132059601057 0.5251948251372414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##그리드 서치 진행"
      ],
      "metadata": {
        "id": "6n6EoM41hH3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = spec_df.drop('name', axis = 1).iloc[:,1:]\n",
        "y = spec_df['target']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.3, stratify=y)\n",
        "model = ExtraTreesClassifier()\n",
        "# 하이퍼파라미터 튜닝을 위한 그리드 서치\n",
        "param_grid = {\n",
        "    'n_estimators': [400,500,600],\n",
        "    'max_depth': [30,40],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 파라미터로 모델 훈련\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 예측\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# 정확도 계산\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"최적의 파라미터: {grid_search.best_params_}\")\n",
        "print(f\"훈련 세트 정확도: {train_accuracy:.3f}\")\n",
        "print(f\"테스트 세트 정확도: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mw9pXLOgfmF",
        "outputId": "89f15891-7ef1-4f30-f5ca-0fc6251f8fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "최적의 파라미터: {'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 400}\n",
            "훈련 세트 정확도: 0.985\n",
            "테스트 세트 정확도: 0.535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# auto ML"
      ],
      "metadata": {
        "id": "KUyls1Xnwa8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = spec_df.drop('name', axis = 1).iloc[:,1:]\n",
        "\n",
        "y = spec_df['target']\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "minmax = scaler.fit_transform(X)\n",
        "minmax_df = pd.DataFrame(minmax, columns = X.columns)\n",
        "X['target'] = spec_df['target']"
      ],
      "metadata": {
        "id": "boEVZrcwwVLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = setup(data = X, target='target', train_size = 0.8, verbose = True, data_split_shuffle=True,session_id= 100)"
      ],
      "metadata": {
        "id": "tdQ34DJiwinL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_model = compare_models(fold=10, round=3, sort=\"Accuracy\", n_select=5)"
      ],
      "metadata": {
        "id": "4NMmywozwkMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blend3 = blend_models(estimator_list = top_5_model, round =4, choose_better=True, optimize='Accuracy')"
      ],
      "metadata": {
        "id": "oAktbJshwlcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(blend3)"
      ],
      "metadata": {
        "id": "mFvsg13ywnYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict_model(final_model, data = test_df.iloc[:,1:])"
      ],
      "metadata": {
        "id": "HTOezd1bwog9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}