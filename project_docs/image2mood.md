# image2mood

## Blip  

Vision Transformer와 Text Transformer간의 상호작용이 핵심인 모델  
두 트랜스포머의 출력이 결합되어 이미지와 텍스트 간의 관계 학습  

![Blip 모델 설명](https://github.com/Taeyoungleee/Dacon-dielectric-prediction/assets/113446739/0e72c2b2-a0d2-439a-b39f-400b56de58c6)  

------------------------------------------------------------------------------------------  
### 데이터  

[Blip 모델 이용 데이터 원본](https://huggingface.co/datasets/visheratin/laion-coco-nllb)  

+ 전체 데이터 수  
89만장  

+ 이용방식  
이미지 링크에서 이미지를 가져와 이미지 caption과 함께 이용 및 학습  

-> **train, valid = 8 : 2 비율**로 학습  

------------------------------------------------------------------------------------------  
### 과정  

1. blip-image-captioning-base 모델 불러오기  

2. Adam 옵티마이저 이용  

3. Image 224 x 224로 통일  

4. 예측 텍스트 및 실제 텍스트 간의 차이 loss 출력  

------------------------------------------------------------------------------------------  
### 결과  

+ 코랩 환경에서 돌리기엔 한계가 명확  
+ 전체 데이터의 1%만 이용해 학습을 진행해도 약 10시간 이상 소요  
+ 1%의 데이터로는 출력되는 텍스트의 결과가 매우 부정확해 이미 학습된 모델을 이용  
