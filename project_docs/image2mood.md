# image2mood

------------------------------------------------------------------------------------------  
## 프로젝트 배경  

+ **현대 사회에서의 음악의 역할**  
음악은 일상 속 어디든 존재하는 필수 요소로 자리 잡음
현대인은 음악으로 지친 감정을 위로 받고 스트레스 관리 및 삶의 원동력으로 삼기도 함

+ **기존 음악 추천 방식**  
1. 협업 필터링  
> 사용자가 전에 구매하였거나 항목을 평가하는 것과 같은 과거의 행동뿐 아니라 다른 사용자들의 행동을 통하여 모델을 구축하고 사용자가 관심있어 할만한 항목을 예측하는 방법  

2. 콘텐츠 기반 필터링  
> 유사한 속성을 가지는 추가적인 아이템을 추천하기 위하여 일련의 별개 아이템 특성을 사용하는 접근 방법  

대부분의 음원 플랫폼들이 이용하는 방식(FLO, Melon, Bugs 등등..)  

+ **프로젝트 목표**
기존의 추천 시스템 방식과 다른 방향으로 접근한 이미지를 기반으로한 새로운 추천 시스템 장르 개발  
사용자의 상황 or 분위기에 맞는 음악 추천 서비스를 구현  

+ **프로젝트 기대 효과**
이미지를 이용한 음악 추천 시스템은 사용자의 경험을 개선, 이용자의 표정 or 상황 이미지를 선택하여 그 이미지에 맞는 음악 추천받을 수 있으며, 이를 통해 좋아하는 음악을 더 쉽게 찾을 수 있음

당장 듣고싶은 노래가 없을때, 해당 기술을 통해 분위기에 맞는 음악을 들을 수 있음  

어떤 노래를 재생해 비슷한 종류의 노래를 추천받는 방식이 아닌 이미지를 활용한 새로운 방식으로 음악을 즐기는 재미있고 새로운 경험 가능  

특정 이미지에 대한 개인의 감정이나 추억을 음악으로 연결해줌으로써 더 깊은 정서적 연결을 만들어냄  

소셜 미디어에 공유하는 사진에 맞는 음악을 추천해주는 기능으로 서비스 활용도 가능  

------------------------------------------------------------------------------------------ 
## Blip  

Vision Transformer와 Text Transformer간의 상호작용이 핵심인 모델  
두 트랜스포머의 출력이 결합되어 이미지와 텍스트 간의 관계 학습  

![Blip 모델 설명](https://github.com/Taeyoungleee/Dacon-dielectric-prediction/assets/113446739/0e72c2b2-a0d2-439a-b39f-400b56de58c6)  

------------------------------------------------------------------------------------------  
### 데이터  

[Blip 모델 이용 데이터 원본](https://huggingface.co/datasets/visheratin/laion-coco-nllb)  

+ 전체 데이터 수  
89만장  

+ 이용방식  
이미지 링크에서 이미지를 가져와 이미지 caption과 함께 이용 및 학습  

-> **train, valid = 8 : 2 비율**로 학습  

------------------------------------------------------------------------------------------  
### 과정  

1. blip-image-captioning-base 모델 불러오기  

2. Adam 옵티마이저 이용  

3. Image 224 x 224로 통일  

4. 예측 텍스트 및 실제 텍스트 간의 차이 loss 출력  

------------------------------------------------------------------------------------------  
### 결과  

+ 코랩 환경에서 돌리기엔 한계가 명확  
+ 전체 데이터의 1%만 이용해 학습을 진행해도 약 10시간 이상 소요  
+ 1%의 데이터로는 출력되는 텍스트의 결과가 매우 부정확해 이미 학습된 모델을 이용  
